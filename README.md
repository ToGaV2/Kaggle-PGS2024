# Kaggle-PGS2024
Kaggle Playground Competition Notebooks from Series 4 (2024). These competitions are conducted on synthetic data generated by GAN networks. Since the data is machine generated prior to the split (training set and testing set), and there are always observations that could not exist in the real world, this is a fairly pure data science competition. The data comes with outliers, noise, and observations with disqualifying inputs. Subject Matter Expertise yields little to no advantage because the training and testing data are machine generated, from the same model, and then split (single corpus of data). Signal is often integrated into unfeasible observations  (I.E. the V8 gasoline powered Teslas in Episode 9.). Competition organizers routinely introduce distracting information intentionally (outliers, repopulating a variable from a uniform distibution, adding a random error to a column of variables, etc.). 

## Results:
- Episode 8: Binary Classification of Poisonous Mushrooms (Ranked 174 / 2422 Teams - Top 8%)
- Episode 9: Regression of Used Car Prices (Ranked 209 / 3066 Teams - Top 7%)
- Episode 10: Loan Approval Prediction (Ranked 205 / 3858 Teams - Top 6%)
- Episode 11: Exploring Mental Health Data (Ranked 57 / 2685 Teams - Top 3%)

## Scoring:
The competition datasets are typically between 200,000 and 500,000 observations. The training set is roughly 60% to 75% of the generated data, and the competition is scored on the remainder. During the competition the public leaderboard is comprised on roughly 1/5th of the hidden testing set. The private leaderboard consists of the remaining 4/5th of the testing set. The final results come from the private leaderboard and are posted after the competition. Said another way, the private leaderboard is used to determine the winner and final rankings.

## Notebook Names:
These notebooks tend to adhere to a naming convention of S4E11 meaning S***series number***E***episode number*** and the type of algorithm run in the notebook. There are examples of Catboost (CAT), XGBoost (XGB), Light Gradient Boosting Machines (LGBM), Neural Networks (NN), Histogram Gradient Boosting (HGBR & HGBC), classical Linear Regression (LM), generalized additive regression models (GAM), and much exploratory data analysis (EDA). Models are often tuned with the Optuna package, but there are examples of manual grid search (*for* loops) here too.  Hopefully, these examples help somebody out at some point. Thanks for taking a look.

### Corroboration of these results can be found on my Kaggle Profile here: https://www.kaggle.com/toddgardiner
